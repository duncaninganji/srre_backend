{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Summarization_task_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "73e80d09d1ae4988ac7b23a588e8b526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d387da710b0a46299efef4e8cd276c83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c9821d81cb74c74ac4cff2f5dfcb6c7",
              "IPY_MODEL_59072f77ccd64131b7e261eca1b768c8",
              "IPY_MODEL_447991e2cf384ad494b8ddd4a6b1d365"
            ]
          }
        },
        "d387da710b0a46299efef4e8cd276c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "column",
            "width": "50%",
            "min_width": null,
            "border": null,
            "align_items": "center",
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "flex",
            "left": null
          }
        },
        "0c9821d81cb74c74ac4cff2f5dfcb6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b707b37206af4b0db6cb7bd6f59fc9d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "<center>\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svg alt='Hugging Face'>\n<br>\n<b>The AI community building the future</b>\n<br>\nImmediately click login after typing your password or it might be stored in plain text in this notebook file.\n</center>",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15ccfd7d8a9145e9b822ebed69a20146"
          }
        },
        "59072f77ccd64131b7e261eca1b768c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a3c5b50422a84cffbab3a4a51c039603",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b6eb1d2a11724db882e0852bc99eadcf",
              "IPY_MODEL_939486ade18347fdac3313a71570c154"
            ]
          }
        },
        "447991e2cf384ad494b8ddd4a6b1d365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_5273d0ee35674161b0c057ed9f7aa206",
            "_dom_classes": [],
            "description": "Login",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_075577745cd2477cbbe4c046b60af6d8",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "b707b37206af4b0db6cb7bd6f59fc9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15ccfd7d8a9145e9b822ebed69a20146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3c5b50422a84cffbab3a4a51c039603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b6eb1d2a11724db882e0852bc99eadcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_8181d63aa8f9442a8fa618d7e4271472",
            "_dom_classes": [],
            "description": "Username:",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_027a7d5e950941e2969f3c90d16c2763"
          }
        },
        "939486ade18347fdac3313a71570c154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "PasswordView",
            "style": "IPY_MODEL_5b96eab7c00b4fc798f4536db3e17b8f",
            "_dom_classes": [],
            "description": "Password:",
            "_model_name": "PasswordModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_362a9ce88cd64eb9bda63fde89b13306"
          }
        },
        "5273d0ee35674161b0c057ed9f7aa206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "075577745cd2477cbbe4c046b60af6d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8181d63aa8f9442a8fa618d7e4271472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "027a7d5e950941e2969f3c90d16c2763": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b96eab7c00b4fc798f4536db3e17b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "362a9ce88cd64eb9bda63fde89b13306": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6abc45370b54ae592ca96d289ec3550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf56ec1da36f4f4ba9bc091e67e116a6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f1a726683152444394aba081b8421937",
              "IPY_MODEL_ddb295ab10cc4dc8b43a8ecdf87f6ba4",
              "IPY_MODEL_d36973d88bdd4097847b800963617a41"
            ]
          }
        },
        "bf56ec1da36f4f4ba9bc091e67e116a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f1a726683152444394aba081b8421937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0b0d0f9046e41a2ba3fc840bef60e3e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55c6a49cdfc44a73a478c912960610e6"
          }
        },
        "ddb295ab10cc4dc8b43a8ecdf87f6ba4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ab8211615c1747b1b7350ed5161c512d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1385,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1385,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8bf6932f2fc44d22827a73ba9bd1adf5"
          }
        },
        "d36973d88bdd4097847b800963617a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_610c561f4b9f481eaf10a6bc740e3916",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.35k/1.35k [00:00&lt;00:00, 54.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c1c9f7741ce4d18b26a7536628e1d7f"
          }
        },
        "e0b0d0f9046e41a2ba3fc840bef60e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55c6a49cdfc44a73a478c912960610e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab8211615c1747b1b7350ed5161c512d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8bf6932f2fc44d22827a73ba9bd1adf5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "610c561f4b9f481eaf10a6bc740e3916": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c1c9f7741ce4d18b26a7536628e1d7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "532755844e8c4175b7d062f0924cfd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c073038cbff845c5ab26b36600a655ac",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cba78855e6de433b8392fc1cf4ee6327",
              "IPY_MODEL_9866f66d9f494b0698a5dd1e68046efe",
              "IPY_MODEL_54c9e54b88134f1db61767506cc81850"
            ]
          }
        },
        "c073038cbff845c5ab26b36600a655ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cba78855e6de433b8392fc1cf4ee6327": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83aae670e561443e88495a851c2c12a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1820eacf17814f78bc6191ad7955ae29"
          }
        },
        "9866f66d9f494b0698a5dd1e68046efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_703a8fcb675d48e8899d3ff0a443c0e8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242085627,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242085627,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a7460a06987451da49ed52ff0a6522b"
          }
        },
        "54c9e54b88134f1db61767506cc81850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a94a7f20e50e4e42afbb0ffaa750c8b7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 231M/231M [00:05&lt;00:00, 54.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_339a10d3f5d44544823177de6754316f"
          }
        },
        "83aae670e561443e88495a851c2c12a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1820eacf17814f78bc6191ad7955ae29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "703a8fcb675d48e8899d3ff0a443c0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a7460a06987451da49ed52ff0a6522b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a94a7f20e50e4e42afbb0ffaa750c8b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "339a10d3f5d44544823177de6754316f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "317226337f104f85838f149f5a3120cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61ae2a271dac40b5a5bbb1c90699e041",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbc6a69d5e4b419d8e43ed4d9698f936",
              "IPY_MODEL_a03e9de71e91458d8339fde7d79733fc",
              "IPY_MODEL_3cb35c3fa66c49eca3f7f1676280954f"
            ]
          }
        },
        "61ae2a271dac40b5a5bbb1c90699e041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbc6a69d5e4b419d8e43ed4d9698f936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f125ef60ec9e4aacb97085133454e528",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9f5c4a23ef744790af7e3b15a56a909d"
          }
        },
        "a03e9de71e91458d8339fde7d79733fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d884104cd584ab5b7d567c5ce25c2ba",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1924,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1924,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2a057ea455c4452fa944b36beaa3edd8"
          }
        },
        "3cb35c3fa66c49eca3f7f1676280954f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b18cc73838ce455c9f3fbf4537b30ed9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.88k/1.88k [00:00&lt;00:00, 75.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee4be3f989c049b5a1903a4c47c6b1f9"
          }
        },
        "f125ef60ec9e4aacb97085133454e528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9f5c4a23ef744790af7e3b15a56a909d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6d884104cd584ab5b7d567c5ce25c2ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2a057ea455c4452fa944b36beaa3edd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b18cc73838ce455c9f3fbf4537b30ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee4be3f989c049b5a1903a4c47c6b1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb3e420ed9564066b5cd48528a5d2c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7983487b932b48f5b4f538142c76cfe5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fd25fb426f214fd293d7230185bb4042",
              "IPY_MODEL_bedf5a64f9fe4daf90132bc9557317c0",
              "IPY_MODEL_7c86ec259e64456e8a3405cfd8906de6"
            ]
          }
        },
        "7983487b932b48f5b4f538142c76cfe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fd25fb426f214fd293d7230185bb4042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95e922a2d4214db380adfffea87f336b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b20686d333c84c80a3c33b43f70fb8a6"
          }
        },
        "bedf5a64f9fe4daf90132bc9557317c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5e94c5644cf4afeb24e67301040fcae",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1387489,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1387489,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c373476e3214d9eac1989ab205e0915"
          }
        },
        "7c86ec259e64456e8a3405cfd8906de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d7db422f91f4a399cbfcd9449da26df",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.32M/1.32M [00:00&lt;00:00, 2.65MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_875f1fbfe90b4d9ca4bcb222ecf951d0"
          }
        },
        "95e922a2d4214db380adfffea87f336b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b20686d333c84c80a3c33b43f70fb8a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5e94c5644cf4afeb24e67301040fcae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c373476e3214d9eac1989ab205e0915": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d7db422f91f4a399cbfcd9449da26df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "875f1fbfe90b4d9ca4bcb222ecf951d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9d93d9b665c4664b467d297200e2b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d6af24d24662465e9ef20822201f0aa2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_528a460d0eaa411e811f9778a953ab0b",
              "IPY_MODEL_9ec61a911fb940fa8532df6c13de9b79",
              "IPY_MODEL_117f45d6762549728a3ae825aef53e4d"
            ]
          }
        },
        "d6af24d24662465e9ef20822201f0aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "528a460d0eaa411e811f9778a953ab0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a19411f6bb9463aa7fa084b78a58bd0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9966b85fda9345558e6df2106ae88ab7"
          }
        },
        "9ec61a911fb940fa8532df6c13de9b79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_44fcbec5422145dca4952e3dc800c15b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1786,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1786,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ca7ab4997ce74815b868c1234eefd771"
          }
        },
        "117f45d6762549728a3ae825aef53e4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_68b85ca392d84fd2b4bb0c64578371e1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.74k/1.74k [00:00&lt;00:00, 66.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8234684421db4022a20b2140384c3bae"
          }
        },
        "5a19411f6bb9463aa7fa084b78a58bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9966b85fda9345558e6df2106ae88ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44fcbec5422145dca4952e3dc800c15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ca7ab4997ce74815b868c1234eefd771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "68b85ca392d84fd2b4bb0c64578371e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8234684421db4022a20b2140384c3bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuZJfCcNGFWn",
        "outputId": "5153af23-8d93-4287-80f1-7bb857ab539b"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Nov  2 02:00:05 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.29.05    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    22W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2j-eAHSGXWQ",
        "outputId": "e33182a9-5c1d-49a0-f8d8-0b0ed2ac4740"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwhpQbJbGaiG",
        "outputId": "30282bb8-fb25-452e-fb9d-81a5deb0e833"
      },
      "source": [
        "pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-mm_4yqbd\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-mm_4yqbd\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.19)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (3.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.13.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.13.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.13.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.13.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.13.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.13.0.dev0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObwJC-euGc7z",
        "outputId": "117278ab-22ce-47e5-c968-cb9c0f89e69a"
      },
      "source": [
        "! pip install datasets  rouge-score nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.14.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (0.12.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267,
          "referenced_widgets": [
            "73e80d09d1ae4988ac7b23a588e8b526",
            "d387da710b0a46299efef4e8cd276c83",
            "0c9821d81cb74c74ac4cff2f5dfcb6c7",
            "59072f77ccd64131b7e261eca1b768c8",
            "447991e2cf384ad494b8ddd4a6b1d365",
            "b707b37206af4b0db6cb7bd6f59fc9d6",
            "15ccfd7d8a9145e9b822ebed69a20146",
            "a3c5b50422a84cffbab3a4a51c039603",
            "b6eb1d2a11724db882e0852bc99eadcf",
            "939486ade18347fdac3313a71570c154",
            "5273d0ee35674161b0c057ed9f7aa206",
            "075577745cd2477cbbe4c046b60af6d8",
            "8181d63aa8f9442a8fa618d7e4271472",
            "027a7d5e950941e2969f3c90d16c2763",
            "5b96eab7c00b4fc798f4536db3e17b8f",
            "362a9ce88cd64eb9bda63fde89b13306"
          ]
        },
        "id": "I58M4fbNGfum",
        "outputId": "0ef0a96d-9805-4c71-f199-1c450d292f1c"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73e80d09d1ae4988ac7b23a588e8b526",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(HTML(value=\"<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP0Q1vrSGjd4",
        "outputId": "f16af8f2-005d-4fe6-d79c-46b9351b59bf"
      },
      "source": [
        "! sudo apt-get install git-lfs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1KoswGFGkpe",
        "outputId": "fa795d0c-ce38-4f15-e048-8929733eeec7"
      },
      "source": [
        "import transformers\n",
        "\n",
        "print(transformers.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.13.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4tO6cD1Gs6c"
      },
      "source": [
        "model_checkpoint = \"t5-small\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Tjz8_QGuaK"
      },
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "\n",
        "metric = load_metric(\"rouge\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI6gm_cdGvyt",
        "outputId": "556046e9-7431-4a3c-eb2d-231fb14b952f"
      },
      "source": [
        "metric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
              "Calculates average rouge scores for a list of hypotheses and references\n",
              "Args:\n",
              "    predictions: list of predictions to score. Each predictions\n",
              "        should be a string with tokens separated by spaces.\n",
              "    references: list of reference for each prediction. Each\n",
              "        reference should be a string with tokens separated by spaces.\n",
              "    rouge_types: A list of rouge types to calculate.\n",
              "        Valid names:\n",
              "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
              "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
              "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
              "\"`.\n",
              "        See details in https://github.com/huggingface/datasets/issues/617\n",
              "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
              "    use_agregator: Return aggregates if this is set to True\n",
              "Returns:\n",
              "    rouge1: rouge_1 (precision, recall, f1),\n",
              "    rouge2: rouge_2 (precision, recall, f1),\n",
              "    rougeL: rouge_l (precision, recall, f1),\n",
              "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
              "Examples:\n",
              "\n",
              "    >>> rouge = datasets.load_metric('rouge')\n",
              "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
              "    >>> references = [\"hello there\", \"general kenobi\"]\n",
              "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
              "    >>> print(list(results.keys()))\n",
              "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
              "    >>> print(results[\"rouge1\"])\n",
              "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
              "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
              "    1.0\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z617XN2WGxud"
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyUxULyHGy8M"
      },
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULsKtZiXG0OE"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAs8Eix8G1kw"
      },
      "source": [
        "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vac2wBGiG45Z"
      },
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples_text):\n",
        "    inputs = [prefix + str(doc) for doc in examples_text]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "    return model_inputs\n",
        "def preprocess_function_label(examples_labels):\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples_labels, max_length=max_target_length, truncation=True)\n",
        "    return labels\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-T4guo9HG_k"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import ast\n",
        "from matplotlib.pyplot import figure\n",
        "from collections import defaultdict\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW9Uj4DzHJNp",
        "outputId": "2bdb18b9-ac2b-40be-bc74-49a08bab3032"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MazMim4fHLJF",
        "outputId": "f53e0064-2fe8-4f76-968a-039ba101b3f7"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1mJT4TPSWcJZRvzDpNcBdnPy-NQmE9lNk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mJT4TPSWcJZRvzDpNcBdnPy-NQmE9lNk\n",
            "To: /content/augmented_recipes.csv\n",
            "100% 501M/501M [00:01<00:00, 292MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lc9OvOsTHO9a"
      },
      "source": [
        "import pandas as pd\n",
        "model_data_df = pd.read_csv('augmented_recipes.csv', index_col=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvOcigYsHQ55"
      },
      "source": [
        "train_texts=model_data_df['steps']\n",
        "train_labels_list=model_data_df['tags']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "vw_1kZVRr-KH",
        "outputId": "84648e97-ed7d-4017-cdc0-08e3942cbe03"
      },
      "source": [
        "model_data_df['steps'][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Preheat oven to 425 degrees F.\\nPress dough into the bottom and sides of a 12 inch pizza pan.\\nBake for 5 minutes until set but not browned.\\nCut sausage into small pieces.\\nWhisk eggs and milk in a bowl until frothy.\\nSpoon sausage over baked crust and sprinkle with cheese.\\nPour egg mixture slowly over sausage and cheese.\\nS& P to taste.\\nBake 15-20 minutes or until eggs are set and crust is brown.'"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rixRyTbbHSkt"
      },
      "source": [
        "train_labels=[]\n",
        "for i in train_labels_list:\n",
        "    tags_summary=i.replace(',','').replace('\\'','').replace('[','').replace(']','')\n",
        "    train_labels.append(tags_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLAiaLpiHUav"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_steps, test_steps, train_tags, test_tags = train_test_split(train_texts, train_labels, test_size=0.20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7DXcyl1HWRK"
      },
      "source": [
        "train_encodings=preprocess_function(train_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fXL6QMgHjBX"
      },
      "source": [
        "train_tags_encodings=preprocess_function_label(train_tags)\n",
        "train_label_encodings=train_tags_encodings['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY5qqJt6HlBz"
      },
      "source": [
        "test_encodings=preprocess_function(test_steps)\n",
        "test_tags_encodings=preprocess_function_label(test_tags)\n",
        "test_label_encodings=test_tags_encodings['input_ids']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyVUYx2NHmye"
      },
      "source": [
        "\n",
        "class FoodComDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = FoodComDataset(train_encodings, train_label_encodings)\n",
        "val_dataset = FoodComDataset(test_encodings, test_label_encodings)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LrJED0oHpHt"
      },
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELxB6q0NHrVB"
      },
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB0yx_ExHsyk"
      },
      "source": [
        "batch_size = 10\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum-proplus\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=20,\n",
        "    predict_with_generate=True,\n",
        "    push_to_hub=True,\n",
        "    fp16=True,\n",
        "    eval_accumulation_steps=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeRLrkB6H5Yh"
      },
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_S4ewkKH7yw"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVhXOQgwH99n",
        "outputId": "dc2bfbcc-091e-4279-eac8-f29b7e29a3c3"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/t5-small-finetuned-xsum-proplus is already a clone of https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "Using amp fp16 backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QwrZWEKXsYB",
        "outputId": "990b32fe-47aa-4aa3-9dee-3c1351615e4c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfsnYaWEXu6_",
        "outputId": "969c9cf9-95db-4932-8b81-94eae756b7d5"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('corpus')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NUToD9ZXIAN6",
        "outputId": "3df3d749-48c8-4236-eaf4-c307da36fa5c"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 178748\n",
            "  Num Epochs = 20\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 357500\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='357501' max='357500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [357500/357500 16:41:16, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.112600</td>\n",
              "      <td>0.954671</td>\n",
              "      <td>38.804800</td>\n",
              "      <td>22.345000</td>\n",
              "      <td>33.817800</td>\n",
              "      <td>33.812000</td>\n",
              "      <td>18.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.945700</td>\n",
              "      <td>0.818165</td>\n",
              "      <td>40.455600</td>\n",
              "      <td>23.730400</td>\n",
              "      <td>35.520400</td>\n",
              "      <td>35.516400</td>\n",
              "      <td>18.273100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.864100</td>\n",
              "      <td>0.754082</td>\n",
              "      <td>41.806300</td>\n",
              "      <td>24.592600</td>\n",
              "      <td>36.460600</td>\n",
              "      <td>36.451800</td>\n",
              "      <td>18.476600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.813200</td>\n",
              "      <td>0.716817</td>\n",
              "      <td>41.965900</td>\n",
              "      <td>24.741900</td>\n",
              "      <td>36.829600</td>\n",
              "      <td>36.827400</td>\n",
              "      <td>18.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.785400</td>\n",
              "      <td>0.691219</td>\n",
              "      <td>42.227300</td>\n",
              "      <td>24.909200</td>\n",
              "      <td>37.112000</td>\n",
              "      <td>37.107300</td>\n",
              "      <td>18.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.766100</td>\n",
              "      <td>0.672171</td>\n",
              "      <td>42.813300</td>\n",
              "      <td>25.417300</td>\n",
              "      <td>37.697900</td>\n",
              "      <td>37.691700</td>\n",
              "      <td>18.380600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.739700</td>\n",
              "      <td>0.659200</td>\n",
              "      <td>44.070700</td>\n",
              "      <td>26.657300</td>\n",
              "      <td>38.743000</td>\n",
              "      <td>38.739000</td>\n",
              "      <td>18.235500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.723000</td>\n",
              "      <td>0.647439</td>\n",
              "      <td>43.527400</td>\n",
              "      <td>26.367700</td>\n",
              "      <td>38.294800</td>\n",
              "      <td>38.289300</td>\n",
              "      <td>18.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.714100</td>\n",
              "      <td>0.638523</td>\n",
              "      <td>43.870000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>38.746600</td>\n",
              "      <td>38.743000</td>\n",
              "      <td>18.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.704800</td>\n",
              "      <td>0.631282</td>\n",
              "      <td>44.403200</td>\n",
              "      <td>26.967700</td>\n",
              "      <td>39.172800</td>\n",
              "      <td>39.170800</td>\n",
              "      <td>18.261500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.695600</td>\n",
              "      <td>0.624862</td>\n",
              "      <td>44.217700</td>\n",
              "      <td>26.759700</td>\n",
              "      <td>39.039400</td>\n",
              "      <td>39.038200</td>\n",
              "      <td>18.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.680100</td>\n",
              "      <td>0.620151</td>\n",
              "      <td>44.199000</td>\n",
              "      <td>26.814600</td>\n",
              "      <td>39.030800</td>\n",
              "      <td>39.031200</td>\n",
              "      <td>18.374900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.684900</td>\n",
              "      <td>0.615456</td>\n",
              "      <td>44.413700</td>\n",
              "      <td>26.863100</td>\n",
              "      <td>39.170900</td>\n",
              "      <td>39.168600</td>\n",
              "      <td>18.334300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.675800</td>\n",
              "      <td>0.612161</td>\n",
              "      <td>44.401100</td>\n",
              "      <td>26.978200</td>\n",
              "      <td>39.220200</td>\n",
              "      <td>39.216300</td>\n",
              "      <td>18.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.609685</td>\n",
              "      <td>44.473200</td>\n",
              "      <td>27.065900</td>\n",
              "      <td>39.301600</td>\n",
              "      <td>39.298900</td>\n",
              "      <td>18.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.672400</td>\n",
              "      <td>0.607404</td>\n",
              "      <td>44.823800</td>\n",
              "      <td>27.255900</td>\n",
              "      <td>39.582000</td>\n",
              "      <td>39.576400</td>\n",
              "      <td>18.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.672700</td>\n",
              "      <td>0.605206</td>\n",
              "      <td>44.698700</td>\n",
              "      <td>27.227000</td>\n",
              "      <td>39.507500</td>\n",
              "      <td>39.502000</td>\n",
              "      <td>18.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.670300</td>\n",
              "      <td>0.604301</td>\n",
              "      <td>45.039000</td>\n",
              "      <td>27.494100</td>\n",
              "      <td>39.765900</td>\n",
              "      <td>39.763700</td>\n",
              "      <td>18.284000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.667100</td>\n",
              "      <td>0.603407</td>\n",
              "      <td>44.720900</td>\n",
              "      <td>27.231100</td>\n",
              "      <td>39.488600</td>\n",
              "      <td>39.485000</td>\n",
              "      <td>18.334300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2794' max='4469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2794/4469 11:17 < 06:46, 4.12 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-500/special_tokens_map.json\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/special_tokens_map.json\n",
            "Several commits (3) will be pushed upstream.\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-17500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-1000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-1000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-1000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-1500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-1500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-1500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-1000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-2000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-2000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-2000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-1500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-2500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-2500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-2500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-2000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-3000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-3000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-3000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-2500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-3500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-3500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-3500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-3000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-4000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-4000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-4000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-3500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-4500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-4500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-4500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-4000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-5000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-5000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-5000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-5000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-4500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-5500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-5500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-5500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-5500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-5000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-6000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-6000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-6000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-6000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-6000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-5500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-6500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-6500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-6500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-6500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-6500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-6000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-7000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-7000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-7000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-7000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-7000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-6500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-7500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-7500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-7500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-7500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-7500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-7000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-8000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-8000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-8000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-8000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-8000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-7500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-8500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-8500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-8500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-8500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-8500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-8000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-9000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-9000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-9000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-9000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-9000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-8500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-9500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-9500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-9500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-9500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-9500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-9000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-10000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-10000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-10000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-10000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-10000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-9500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-10500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-10500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-10500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-10500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-10500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-10000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-11000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-11000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-11000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-11000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-11000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-10500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-11500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-11500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-11500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-11500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-11500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-11000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-12000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-12000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-12000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-12000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-12000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-11500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-12500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-12500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-12500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-12500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-12500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-12000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-13000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-13000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-13000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-13000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-13000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-12500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-13500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-13500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-13500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-13500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-13500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-13000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-14000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-14000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-14000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-14000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-14000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-13500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-14500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-14500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-14500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-14500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-14500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-14000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-15000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-15000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-15000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-15000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-15000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-14500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-15500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-15500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-15500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-15500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-15500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-15000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-16000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-16000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-16000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-16000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-16000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-15500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-16500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-16500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-16500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-16500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-16500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-16000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-17000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-17000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-17000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-17000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-17000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-16500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-17500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-17500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-17500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-17500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-17500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-17000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-18000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-18000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-18000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-18000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-18000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-17500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-18500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-18500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-18500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-18500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-18500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-18000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-19000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-19000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-19000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-19000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-19000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-18500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-19500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-19500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-19500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-19500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-19500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-19000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-20000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-20000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-20000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-20000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-20000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-19500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-20500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-20500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-20500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-20500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-20500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-20000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-21000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-21000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-21000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-21000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-21000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-20500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-21500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-21500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-21500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-21500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-21500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-21000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-22000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-22000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-22000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-22000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-22000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-21500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-22500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-22500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-22500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-22500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-22500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-22000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-23000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-23000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-23000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-23000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-23000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-22500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-23500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-23500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-23500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-23500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-23500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-23000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-24000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-24000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-24000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-24000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-24000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-23500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-24500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-24500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-24500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-24500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-24500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-24000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-25000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-25000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-25000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-25000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-25000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-24500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-25500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-25500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-25500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-25500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-25500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-25000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-26000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-26000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-26000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-26000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-26000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-25500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-26500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-26500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-26500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-26500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-26500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-26000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-27000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-27000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-27000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-27000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-27000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-26500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-27500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-27500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-27500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-27500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-27500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-27000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-28000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-28000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-28000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-28000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-28000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-27500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-28500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-28500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-28500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-28500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-28500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-28000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-29000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-29000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-29000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-29000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-29000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-28500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-29500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-29500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-29500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-29500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-29500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-29000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-30000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-30000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-30000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-30000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-30000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-29500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-30500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-30500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-30500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-30500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-30500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-30000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-31000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-31000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-31000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-31000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-31000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-30500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-31500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-31500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-31500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-31500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-31500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-31000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-32000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-32000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-32000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-32000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-32000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-31500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-32500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-32500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-32500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-32500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-32500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-32000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-33000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-33000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-33000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-33000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-33000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-32500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-33500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-33500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-33500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-33500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-33500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-33000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-34000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-34000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-34000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-34000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-34000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-33500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-34500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-34500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-34500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-34500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-34500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-34000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-35000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-35000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-35000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-35000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-35000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-34500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-35500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-35500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-35500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-35500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-35500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-35000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-36000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-36000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-36000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-36000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-36000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-35500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-36500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-36500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-36500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-36500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-36500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-36000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-37000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-37000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-37000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-37000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-37000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-36500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-37500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-37500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-37500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-37500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-37500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-37000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-38000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-38000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-38000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-38000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-38000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-37500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-38500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-38500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-38500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-38500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-38500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-38000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-39000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-39000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-39000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-39000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-39000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-38500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-39500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-39500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-39500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-39500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-39500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-39000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-40000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-40000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-40000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-40000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-40000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-39500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-40500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-40500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-40500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-40500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-40500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-40000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-41000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-41000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-41000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-41000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-41000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-40500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-41500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-41500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-41500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-41500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-41500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-41000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-42000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-42000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-42000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-42000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-42000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-41500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-42500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-42500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-42500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-42500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-42500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-42000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-43000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-43000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-43000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-43000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-43000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-42500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-43500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-43500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-43500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-43500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-43500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-43000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-44000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-44000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-44000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-44000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-44000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-43500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-44500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-44500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-44500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-44500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-44500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-44000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-45000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-45000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-45000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-45000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-45000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-44500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-45500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-45500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-45500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-45500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-45500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-45000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-46000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-46000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-46000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-46000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-46000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-45500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-46500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-46500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-46500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-46500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-46500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-46000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-47000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-47000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-47000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-47000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-47000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-46500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-47500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-47500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-47500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-47500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-47500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-47000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-48000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-48000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-48000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-48000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-48000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-47500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-48500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-48500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-48500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-48500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-48500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-48000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-49000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-49000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-49000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-49000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-49000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-48500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-49500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-49500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-49500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-49500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-49500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-49000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-50000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-50000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-50000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-50000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-50000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-49500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-50500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-50500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-50500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-50500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-50500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-50000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-51000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-51000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-51000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-51000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-51000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-50500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-51500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-51500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-51500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-51500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-51500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-51000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-52000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-52000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-52000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-52000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-52000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-51500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-52500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-52500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-52500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-52500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-52500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-52000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-53000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-53000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-53000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-53000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-53000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-52500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-53500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-53500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-53500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-53500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-53500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-53000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-54000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-54000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-54000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-54000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-54000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-53500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-54500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-54500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-54500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-54500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-54500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-54000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-55000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-55000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-55000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-55000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-55000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-54500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-55500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-55500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-55500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-55500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-55500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-55000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-56000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-56000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-56000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-56000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-56000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-55500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-56500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-56500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-56500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-56500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-56500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-56000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-57000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-57000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-57000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-57000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-57000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-56500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-57500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-57500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-57500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-57500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-57500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-57000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-58000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-58000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-58000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-58000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-58000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-57500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-58500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-58500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-58500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-58500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-58500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-58000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-59000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-59000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-59000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-59000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-59000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-58500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-59500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-59500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-59500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-59500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-59500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-59000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-60000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-60000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-60000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-60000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-60000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-59500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-60500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-60500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-60500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-60500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-60500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-60000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-61000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-61000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-61000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-61000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-61000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-60500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-61500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-61500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-61500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-61500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-61500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-61000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-62000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-62000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-62000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-62000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-62000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-61500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-62500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-62500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-62500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-62500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-62500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-62000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-63000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-63000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-63000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-63000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-63000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-62500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-63500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-63500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-63500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-63500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-63500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-63000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-64000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-64000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-64000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-64000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-64000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-63500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-64500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-64500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-64500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-64500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-64500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-64000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-65000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-65000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-65000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-65000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-65000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-64500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-65500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-65500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-65500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-65500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-65500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-65000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-66000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-66000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-66000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-66000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-66000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-65500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-66500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-66500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-66500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-66500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-66500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-66000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-67000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-67000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-67000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-67000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-67000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-66500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-67500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-67500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-67500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-67500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-67500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-67000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-68000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-68000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-68000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-68000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-68000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-67500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-68500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-68500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-68500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-68500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-68500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-68000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-69000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-69000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-69000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-69000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-69000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-68500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-69500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-69500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-69500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-69500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-69500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-69000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-70000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-70000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-70000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-70000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-70000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-69500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-70500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-70500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-70500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-70500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-70500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-70000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-71000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-71000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-71000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-71000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-71000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-70500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-71500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-71500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-71500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-71500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-71500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-71000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-72000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-72000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-72000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-72000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-72000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-71500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-72500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-72500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-72500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-72500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-72500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-72000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-73000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-73000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-73000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-73000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-73000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-72500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-73500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-73500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-73500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-73500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-73500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-73000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-74000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-74000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-74000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-74000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-74000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-73500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-74500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-74500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-74500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-74500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-74500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-74000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-75000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-75000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-75000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-75000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-75000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-74500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-75500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-75500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-75500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-75500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-75500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-75000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-76000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-76000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-76000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-76000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-76000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-75500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-76500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-76500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-76500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-76500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-76500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-76000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-77000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-77000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-77000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-77000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-77000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-76500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-77500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-77500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-77500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-77500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-77500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-77000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-78000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-78000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-78000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-78000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-78000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-77500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-78500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-78500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-78500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-78500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-78500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-78000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-79000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-79000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-79000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-79000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-79000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-78500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-79500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-79500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-79500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-79500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-79500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-79000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-80000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-80000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-80000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-80000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-80000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-79500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-80500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-80500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-80500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-80500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-80500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-80000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-81000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-81000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-81000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-81000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-81000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-80500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-81500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-81500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-81500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-81500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-81500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-81000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-82000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-82000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-82000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-82000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-82000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-81500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-82500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-82500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-82500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-82500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-82500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-82000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-83000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-83000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-83000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-83000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-83000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-82500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-83500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-83500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-83500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-83500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-83500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-83000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-84000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-84000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-84000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-84000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-84000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-83500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-84500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-84500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-84500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-84500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-84500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-84000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-85000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-85000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-85000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-85000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-85000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-84500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-85500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-85500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-85500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-85500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-85500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-85000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-86000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-86000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-86000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-86000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-86000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-85500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-86500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-86500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-86500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-86500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-86500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-86000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-87000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-87000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-87000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-87000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-87000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-86500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-87500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-87500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-87500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-87500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-87500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-87000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-88000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-88000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-88000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-88000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-88000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-87500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-88500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-88500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-88500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-88500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-88500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-88000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-89000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-89000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-89000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-89000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-89000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-88500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-89500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-89500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-89500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-89500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-89500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-89000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-90000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-90000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-90000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-90000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-90000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-89500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-90500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-90500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-90500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-90500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-90500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-90000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-91000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-91000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-91000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-91000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-91000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-90500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-91500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-91500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-91500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-91500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-91500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-91000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-92000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-92000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-92000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-92000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-92000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-91500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-92500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-92500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-92500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-92500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-92500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-92000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-93000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-93000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-93000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-93000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-93000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-92500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-93500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-93500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-93500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-93500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-93500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-93000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-94000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-94000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-94000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-94000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-94000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-93500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-94500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-94500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-94500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-94500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-94500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-94000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-95000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-95000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-95000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-95000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-95000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-94500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-95500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-95500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-95500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-95500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-95500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-95000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-96000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-96000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-96000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-96000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-96000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-95500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-96500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-96500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-96500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-96500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-96500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-96000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-97000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-97000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-97000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-97000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-97000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-96500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-97500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-97500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-97500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-97500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-97500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-97000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-98000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-98000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-98000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-98000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-98000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-97500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-98500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-98500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-98500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-98500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-98500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-98000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-99000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-99000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-99000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-99000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-99000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-98500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-99500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-99500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-99500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-99500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-99500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-99000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-100000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-100000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-100000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-100000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-100000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-99500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-100500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-100500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-100500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-100500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-100500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-100000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-101000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-101000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-101000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-101000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-101000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-100500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-101500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-101500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-101500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-101500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-101500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-101000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-102000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-102000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-102000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-102000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-102000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-101500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-102500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-102500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-102500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-102500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-102500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-102000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-103000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-103000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-103000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-103000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-103000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-102500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-103500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-103500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-103500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-103500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-103500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-103000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-104000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-104000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-104000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-104000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-104000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-103500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-104500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-104500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-104500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-104500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-104500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-104000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-105000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-105000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-105000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-105000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-105000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-104500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-105500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-105500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-105500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-105500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-105500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-105000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-106000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-106000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-106000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-106000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-106000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-105500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-106500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-106500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-106500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-106500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-106500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-106000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-107000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-107000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-107000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-107000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-107000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-106500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-107500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-107500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-107500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-107500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-107500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-107000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-108000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-108000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-108000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-108000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-108000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-107500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-108500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-108500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-108500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-108500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-108500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-108000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-109000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-109000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-109000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-109000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-109000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-108500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-109500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-109500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-109500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-109500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-109500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-109000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-110000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-110000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-110000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-110000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-110000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-109500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-110500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-110500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-110500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-110500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-110500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-110000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-111000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-111000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-111000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-111000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-111000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-110500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-111500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-111500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-111500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-111500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-111500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-111000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-112000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-112000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-112000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-112000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-112000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-111500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-112500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-112500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-112500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-112500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-112500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-112000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-113000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-113000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-113000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-113000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-113000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-112500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-113500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-113500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-113500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-113500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-113500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-113000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-114000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-114000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-114000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-114000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-114000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-113500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-114500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-114500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-114500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-114500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-114500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-114000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-115000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-115000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-115000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-115000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-115000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-114500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-115500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-115500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-115500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-115500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-115500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-115000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-116000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-116000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-116000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-116000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-116000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-115500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-116500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-116500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-116500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-116500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-116500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-116000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-117000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-117000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-117000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-117000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-117000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-116500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-117500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-117500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-117500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-117500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-117500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-117000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-118000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-118000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-118000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-118000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-118000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-117500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-118500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-118500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-118500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-118500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-118500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-118000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-119000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-119000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-119000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-119000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-119000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-118500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-119500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-119500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-119500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-119500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-119500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-119000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-120000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-120000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-120000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-120000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-120000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-119500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-120500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-120500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-120500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-120500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-120500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-120000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-121000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-121000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-121000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-121000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-121000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-120500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-121500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-121500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-121500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-121500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-121500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-121000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-122000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-122000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-122000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-122000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-122000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-121500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-122500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-122500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-122500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-122500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-122500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-122000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-123000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-123000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-123000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-123000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-123000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-122500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-123500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-123500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-123500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-123500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-123500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-123000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-124000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-124000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-124000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-124000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-124000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-123500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-124500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-124500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-124500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-124500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-124500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-124000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-125000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-125000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-125000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-125000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-125000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-124500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-125500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-125500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-125500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-125500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-125500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-125000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-126000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-126000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-126000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-126000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-126000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-125500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-126500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-126500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-126500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-126500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-126500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-126000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-127000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-127000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-127000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-127000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-127000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-126500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-127500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-127500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-127500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-127500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-127500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-127000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-128000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-128000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-128000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-128000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-128000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-127500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-128500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-128500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-128500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-128500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-128500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-128000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-129000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-129000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-129000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-129000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-129000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-128500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-129500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-129500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-129500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-129500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-129500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-129000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-130000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-130000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-130000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-130000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-130000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-129500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-130500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-130500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-130500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-130500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-130500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-130000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-131000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-131000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-131000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-131000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-131000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-130500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-131500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-131500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-131500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-131500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-131500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-131000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-132000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-132000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-132000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-132000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-132000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-131500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-132500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-132500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-132500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-132500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-132500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-132000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-133000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-133000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-133000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-133000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-133000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-132500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-133500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-133500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-133500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-133500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-133500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-133000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-134000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-134000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-134000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-134000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-134000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-133500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-134500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-134500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-134500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-134500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-134500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-134000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-135000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-135000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-135000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-135000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-135000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-134500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-135500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-135500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-135500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-135500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-135500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-135000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-136000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-136000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-136000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-136000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-136000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-135500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-136500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-136500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-136500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-136500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-136500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-136000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-137000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-137000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-137000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-137000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-137000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-136500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-137500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-137500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-137500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-137500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-137500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-137000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-138000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-138000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-138000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-138000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-138000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-137500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-138500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-138500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-138500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-138500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-138500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-138000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-139000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-139000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-139000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-139000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-139000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-138500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-139500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-139500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-139500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-139500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-139500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-139000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-140000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-140000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-140000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-140000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-140000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-139500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-140500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-140500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-140500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-140500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-140500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-140000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-141000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-141000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-141000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-141000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-141000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-140500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-141500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-141500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-141500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-141500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-141500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-141000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-142000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-142000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-142000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-142000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-142000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-141500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-142500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-142500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-142500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-142500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-142500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-142000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-143000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-143000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-143000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-143000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-143000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-142500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-143500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-143500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-143500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-143500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-143500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-143000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-144000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-144000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-144000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-144000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-144000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-143500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-144500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-144500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-144500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-144500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-144500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-144000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-145000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-145000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-145000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-145000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-145000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-144500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-145500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-145500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-145500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-145500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-145500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-145000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-146000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-146000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-146000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-146000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-146000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-145500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-146500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-146500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-146500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-146500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-146500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-146000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-147000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-147000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-147000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-147000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-147000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-146500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-147500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-147500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-147500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-147500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-147500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-147000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-148000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-148000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-148000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-148000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-148000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-147500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-148500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-148500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-148500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-148500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-148500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-148000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-149000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-149000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-149000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-149000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-149000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-148500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-149500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-149500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-149500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-149500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-149500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-149000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-150000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-150000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-150000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-150000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-150000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-149500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-150500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-150500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-150500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-150500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-150500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-150000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-151000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-151000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-151000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-151000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-151000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-150500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-151500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-151500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-151500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-151500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-151500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-151000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-152000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-152000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-152000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-152000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-152000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-151500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-152500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-152500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-152500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-152500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-152500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-152000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-153000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-153000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-153000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-153000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-153000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-152500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-153500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-153500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-153500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-153500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-153500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-153000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-154000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-154000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-154000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-154000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-154000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-153500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-154500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-154500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-154500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-154500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-154500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-154000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-155000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-155000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-155000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-155000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-155000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-154500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-155500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-155500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-155500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-155500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-155500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-155000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-156000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-156000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-156000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-156000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-156000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-155500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-156500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-156500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-156500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-156500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-156500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-156000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-157000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-157000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-157000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-157000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-157000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-156500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-157500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-157500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-157500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-157500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-157500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-157000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-158000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-158000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-158000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-158000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-158000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-157500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-158500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-158500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-158500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-158500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-158500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-158000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-159000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-159000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-159000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-159000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-159000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-158500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-159500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-159500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-159500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-159500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-159500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-159000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-160000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-160000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-160000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-160000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-160000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-159500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-160500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-160500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-160500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-160500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-160500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-160000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-161000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-161000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-161000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-161000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-161000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-160500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-161500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-161500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-161500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-161500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-161500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-161000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-162000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-162000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-162000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-162000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-162000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-161500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-162500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-162500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-162500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-162500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-162500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-162000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-163000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-163000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-163000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-163000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-163000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-162500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-163500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-163500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-163500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-163500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-163500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-163000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-164000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-164000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-164000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-164000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-164000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-163500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-164500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-164500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-164500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-164500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-164500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-164000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-165000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-165000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-165000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-165000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-165000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-164500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-165500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-165500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-165500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-165500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-165500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-165000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-166000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-166000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-166000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-166000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-166000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-165500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-166500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-166500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-166500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-166500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-166500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-166000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-167000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-167000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-167000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-167000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-167000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-166500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-167500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-167500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-167500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-167500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-167500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-167000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-168000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-168000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-168000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-168000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-168000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-167500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-168500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-168500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-168500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-168500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-168500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-168000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-169000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-169000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-169000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-169000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-169000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-168500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-169500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-169500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-169500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-169500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-169500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-169000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-170000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-170000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-170000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-170000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-170000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-169500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-170500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-170500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-170500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-170500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-170500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-170000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-171000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-171000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-171000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-171000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-171000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-170500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-171500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-171500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-171500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-171500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-171500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-171000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-172000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-172000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-172000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-172000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-172000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-171500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-172500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-172500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-172500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-172500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-172500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-172000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-173000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-173000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-173000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-173000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-173000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-172500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-173500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-173500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-173500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-173500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-173500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-173000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-174000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-174000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-174000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-174000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-174000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-173500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-174500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-174500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-174500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-174500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-174500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-174000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-175000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-175000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-175000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-175000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-175000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-174500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-175500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-175500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-175500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-175500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-175500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-175000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-176000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-176000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-176000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-176000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-176000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-175500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-176500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-176500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-176500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-176500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-176500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-176000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-177000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-177000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-177000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-177000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-177000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-176500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-177500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-177500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-177500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-177500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-177500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-177000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-178000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-178000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-178000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-178000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-178000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-177500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-178500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-178500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-178500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-178500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-178500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-178000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-179000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-179000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-179000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-179000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-179000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-178500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-179500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-179500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-179500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-179500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-179500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-179000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-180000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-180000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-180000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-180000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-180000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-179500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-180500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-180500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-180500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-180500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-180500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-180000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-181000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-181000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-181000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-181000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-181000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-180500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-181500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-181500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-181500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-181500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-181500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-181000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-182000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-182000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-182000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-182000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-182000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-181500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-182500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-182500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-182500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-182500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-182500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-182000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-183000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-183000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-183000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-183000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-183000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-182500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-183500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-183500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-183500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-183500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-183500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-183000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-184000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-184000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-184000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-184000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-184000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-183500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-184500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-184500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-184500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-184500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-184500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-184000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-185000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-185000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-185000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-185000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-185000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-184500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-185500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-185500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-185500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-185500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-185500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-185000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-186000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-186000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-186000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-186000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-186000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-185500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-186500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-186500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-186500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-186500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-186500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-186000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-187000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-187000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-187000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-187000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-187000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-186500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-187500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-187500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-187500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-187500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-187500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-187000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-188000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-188000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-188000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-188000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-188000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-187500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-188500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-188500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-188500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-188500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-188500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-188000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-189000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-189000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-189000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-189000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-189000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-188500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-189500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-189500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-189500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-189500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-189500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-189000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-190000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-190000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-190000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-190000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-190000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-189500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-190500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-190500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-190500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-190500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-190500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-190000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-191000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-191000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-191000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-191000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-191000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-190500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-191500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-191500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-191500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-191500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-191500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-191000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-192000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-192000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-192000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-192000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-192000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-191500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-192500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-192500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-192500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-192500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-192500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-192000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-193000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-193000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-193000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-193000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-193000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-192500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-193500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-193500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-193500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-193500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-193500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-193000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-194000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-194000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-194000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-194000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-194000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-193500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-194500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-194500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-194500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-194500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-194500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-194000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-195000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-195000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-195000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-195000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-195000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-194500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-195500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-195500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-195500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-195500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-195500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-195000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-196000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-196000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-196000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-196000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-196000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-195500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-196500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-196500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-196500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-196500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-196500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-196000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-197000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-197000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-197000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-197000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-197000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-196500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-197500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-197500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-197500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-197500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-197500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-197000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-198000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-198000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-198000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-198000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-198000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-197500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-198500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-198500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-198500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-198500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-198500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-198000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-199000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-199000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-199000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-199000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-199000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-198500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-199500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-199500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-199500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-199500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-199500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-199000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-200000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-200000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-200000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-200000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-200000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-199500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-200500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-200500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-200500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-200500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-200500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-200000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-201000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-201000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-201000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-201000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-201000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-200500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-201500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-201500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-201500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-201500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-201500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-201000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-202000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-202000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-202000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-202000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-202000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-201500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-202500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-202500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-202500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-202500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-202500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-202000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-203000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-203000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-203000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-203000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-203000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-202500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-203500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-203500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-203500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-203500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-203500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-203000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-204000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-204000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-204000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-204000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-204000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-203500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-204500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-204500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-204500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-204500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-204500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-204000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-205000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-205000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-205000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-205000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-205000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-204500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-205500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-205500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-205500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-205500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-205500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-205000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-206000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-206000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-206000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-206000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-206000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-205500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-206500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-206500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-206500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-206500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-206500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-206000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-207000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-207000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-207000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-207000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-207000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-206500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-207500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-207500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-207500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-207500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-207500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-207000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-208000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-208000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-208000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-208000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-208000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-207500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-208500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-208500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-208500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-208500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-208500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-208000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-209000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-209000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-209000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-209000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-209000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-208500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-209500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-209500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-209500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-209500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-209500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-209000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-210000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-210000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-210000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-210000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-210000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-209500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-210500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-210500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-210500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-210500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-210500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-210000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-211000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-211000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-211000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-211000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-211000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-210500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-211500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-211500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-211500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-211500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-211500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-211000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-212000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-212000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-212000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-212000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-212000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-211500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-212500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-212500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-212500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-212500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-212500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-212000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-213000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-213000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-213000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-213000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-213000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-212500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-213500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-213500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-213500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-213500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-213500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-213000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-214000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-214000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-214000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-214000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-214000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-213500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-214500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-214500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-214500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-214500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-214500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-214000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-215000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-215000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-215000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-215000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-215000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-214500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-215500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-215500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-215500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-215500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-215500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-215000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-216000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-216000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-216000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-216000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-216000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-215500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-216500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-216500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-216500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-216500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-216500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-216000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-217000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-217000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-217000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-217000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-217000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-216500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-217500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-217500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-217500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-217500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-217500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-217000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-218000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-218000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-218000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-218000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-218000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-217500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-218500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-218500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-218500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-218500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-218500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-218000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-219000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-219000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-219000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-219000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-219000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-218500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-219500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-219500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-219500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-219500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-219500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-219000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-220000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-220000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-220000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-220000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-220000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-219500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-220500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-220500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-220500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-220500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-220500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-220000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-221000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-221000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-221000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-221000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-221000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-220500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-221500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-221500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-221500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-221500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-221500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-221000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-222000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-222000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-222000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-222000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-222000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-221500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-222500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-222500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-222500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-222500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-222500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-222000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-223000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-223000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-223000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-223000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-223000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-222500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-223500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-223500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-223500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-223500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-223500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-223000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-224000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-224000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-224000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-224000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-224000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-223500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-224500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-224500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-224500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-224500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-224500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-224000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-225000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-225000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-225000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-225000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-225000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-224500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-225500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-225500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-225500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-225500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-225500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-225000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-226000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-226000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-226000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-226000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-226000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-225500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-226500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-226500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-226500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-226500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-226500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-226000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-227000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-227000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-227000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-227000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-227000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-226500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-227500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-227500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-227500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-227500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-227500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-227000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-228000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-228000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-228000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-228000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-228000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-227500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-228500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-228500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-228500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-228500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-228500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-228000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-229000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-229000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-229000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-229000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-229000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-228500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-229500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-229500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-229500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-229500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-229500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-229000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-230000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-230000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-230000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-230000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-230000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-229500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-230500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-230500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-230500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-230500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-230500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-230000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-231000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-231000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-231000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-231000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-231000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-230500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-231500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-231500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-231500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-231500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-231500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-231000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-232000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-232000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-232000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-232000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-232000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-231500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-232500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-232500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-232500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-232500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-232500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-232000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-233000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-233000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-233000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-233000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-233000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-232500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-233500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-233500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-233500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-233500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-233500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-233000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-234000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-234000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-234000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-234000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-234000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-233500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-234500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-234500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-234500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-234500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-234500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-234000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-235000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-235000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-235000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-235000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-235000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-234500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-235500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-235500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-235500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-235500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-235500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-235000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-236000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-236000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-236000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-236000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-236000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-235500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-236500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-236500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-236500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-236500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-236500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-236000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-237000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-237000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-237000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-237000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-237000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-236500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-237500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-237500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-237500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-237500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-237500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-237000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-238000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-238000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-238000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-238000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-238000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-237500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-238500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-238500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-238500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-238500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-238500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-238000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-239000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-239000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-239000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-239000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-239000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-238500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-239500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-239500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-239500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-239500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-239500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-239000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-240000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-240000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-240000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-240000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-240000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-239500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-240500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-240500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-240500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-240500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-240500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-240000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-241000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-241000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-241000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-241000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-241000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-240500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-241500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-241500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-241500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-241500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-241500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-241000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-242000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-242000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-242000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-242000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-242000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-241500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-242500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-242500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-242500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-242500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-242500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-242000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-243000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-243000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-243000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-243000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-243000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-242500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-243500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-243500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-243500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-243500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-243500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-243000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-244000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-244000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-244000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-244000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-244000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-243500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-244500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-244500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-244500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-244500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-244500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-244000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-245000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-245000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-245000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-245000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-245000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-244500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-245500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-245500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-245500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-245500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-245500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-245000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-246000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-246000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-246000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-246000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-246000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-245500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-246500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-246500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-246500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-246500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-246500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-246000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-247000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-247000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-247000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-247000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-247000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-246500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-247500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-247500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-247500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-247500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-247500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-247000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-248000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-248000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-248000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-248000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-248000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-247500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-248500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-248500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-248500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-248500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-248500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-248000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-249000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-249000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-249000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-249000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-249000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-248500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-249500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-249500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-249500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-249500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-249500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-249000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-250000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-250000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-250000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-250000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-250000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-249500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-250500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-250500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-250500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-250500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-250500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-250000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-251000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-251000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-251000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-251000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-251000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-250500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-251500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-251500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-251500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-251500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-251500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-251000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-252000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-252000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-252000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-252000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-252000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-251500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-252500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-252500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-252500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-252500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-252500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-252000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-253000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-253000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-253000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-253000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-253000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-252500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-253500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-253500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-253500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-253500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-253500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-253000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-254000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-254000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-254000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-254000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-254000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-253500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-254500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-254500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-254500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-254500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-254500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-254000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-255000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-255000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-255000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-255000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-255000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-254500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-255500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-255500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-255500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-255500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-255500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-255000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-256000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-256000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-256000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-256000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-256000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-255500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-256500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-256500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-256500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-256500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-256500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-256000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-257000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-257000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-257000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-257000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-257000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-256500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-257500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-257500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-257500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-257500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-257500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-257000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-258000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-258000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-258000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-258000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-258000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-257500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-258500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-258500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-258500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-258500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-258500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-258000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-259000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-259000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-259000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-259000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-259000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-258500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-259500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-259500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-259500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-259500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-259500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-259000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-260000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-260000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-260000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-260000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-260000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-259500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-260500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-260500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-260500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-260500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-260500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-260000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-261000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-261000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-261000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-261000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-261000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-260500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-261500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-261500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-261500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-261500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-261500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-261000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-262000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-262000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-262000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-262000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-262000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-261500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-262500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-262500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-262500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-262500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-262500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-262000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-263000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-263000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-263000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-263000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-263000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-262500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-263500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-263500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-263500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-263500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-263500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-263000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-264000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-264000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-264000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-264000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-264000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-263500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-264500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-264500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-264500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-264500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-264500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-264000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-265000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-265000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-265000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-265000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-265000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-264500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-265500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-265500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-265500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-265500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-265500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-265000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-266000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-266000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-266000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-266000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-266000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-265500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-266500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-266500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-266500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-266500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-266500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-266000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-267000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-267000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-267000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-267000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-267000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-266500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-267500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-267500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-267500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-267500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-267500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-267000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-268000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-268000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-268000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-268000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-268000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-267500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-268500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-268500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-268500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-268500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-268500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-268000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-269000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-269000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-269000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-269000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-269000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-268500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-269500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-269500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-269500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-269500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-269500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-269000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-270000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-270000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-270000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-270000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-270000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-269500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-270500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-270500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-270500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-270500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-270500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-270000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-271000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-271000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-271000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-271000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-271000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-270500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-271500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-271500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-271500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-271500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-271500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-271000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-272000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-272000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-272000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-272000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-272000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-271500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-272500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-272500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-272500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-272500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-272500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-272000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-273000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-273000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-273000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-273000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-273000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-272500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-273500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-273500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-273500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-273500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-273500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-273000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-274000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-274000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-274000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-274000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-274000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-273500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-274500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-274500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-274500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-274500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-274500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-274000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-275000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-275000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-275000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-275000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-275000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-274500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-275500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-275500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-275500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-275500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-275500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-275000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-276000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-276000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-276000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-276000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-276000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-275500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-276500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-276500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-276500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-276500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-276500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-276000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-277000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-277000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-277000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-277000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-277000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-276500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-277500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-277500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-277500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-277500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-277500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-277000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-278000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-278000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-278000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-278000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-278000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-277500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-278500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-278500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-278500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-278500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-278500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-278000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-279000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-279000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-279000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-279000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-279000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-278500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-279500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-279500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-279500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-279500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-279500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-279000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-280000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-280000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-280000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-280000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-280000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-279500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-280500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-280500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-280500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-280500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-280500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-280000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-281000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-281000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-281000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-281000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-281000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-280500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-281500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-281500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-281500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-281500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-281500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-281000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-282000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-282000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-282000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-282000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-282000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-281500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-282500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-282500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-282500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-282500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-282500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-282000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-283000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-283000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-283000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-283000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-283000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-282500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-283500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-283500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-283500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-283500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-283500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-283000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-284000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-284000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-284000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-284000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-284000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-283500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-284500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-284500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-284500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-284500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-284500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-284000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-285000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-285000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-285000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-285000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-285000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-284500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-285500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-285500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-285500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-285500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-285500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-285000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-286000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-286000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-286000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-286000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-286000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-285500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-286500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-286500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-286500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-286500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-286500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-286000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-287000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-287000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-287000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-287000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-287000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-286500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-287500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-287500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-287500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-287500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-287500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-287000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-288000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-288000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-288000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-288000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-288000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-287500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-288500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-288500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-288500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-288500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-288500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-288000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-289000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-289000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-289000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-289000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-289000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-288500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-289500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-289500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-289500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-289500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-289500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-289000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-290000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-290000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-290000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-290000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-290000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-289500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-290500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-290500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-290500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-290500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-290500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-290000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-291000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-291000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-291000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-291000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-291000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-290500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-291500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-291500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-291500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-291500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-291500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-291000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-292000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-292000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-292000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-292000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-292000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-291500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-292500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-292500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-292500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-292500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-292500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-292000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-293000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-293000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-293000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-293000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-293000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-292500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-293500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-293500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-293500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-293500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-293500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-293000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-294000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-294000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-294000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-294000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-294000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-293500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-294500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-294500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-294500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-294500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-294500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-294000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-295000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-295000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-295000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-295000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-295000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-294500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-295500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-295500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-295500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-295500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-295500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-295000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-296000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-296000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-296000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-296000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-296000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-295500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-296500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-296500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-296500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-296500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-296500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-296000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-297000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-297000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-297000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-297000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-297000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-296500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-297500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-297500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-297500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-297500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-297500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-297000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-298000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-298000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-298000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-298000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-298000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-297500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-298500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-298500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-298500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-298500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-298500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-298000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-299000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-299000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-299000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-299000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-299000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-298500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-299500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-299500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-299500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-299500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-299500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-299000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-300000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-300000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-300000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-300000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-300000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-299500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-300500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-300500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-300500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-300500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-300500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-300000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-301000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-301000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-301000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-301000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-301000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-300500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-301500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-301500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-301500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-301500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-301500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-301000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-302000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-302000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-302000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-302000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-302000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-301500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-302500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-302500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-302500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-302500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-302500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-302000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-303000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-303000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-303000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-303000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-303000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-302500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-303500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-303500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-303500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-303500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-303500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-303000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-304000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-304000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-304000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-304000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-304000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-303500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-304500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-304500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-304500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-304500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-304500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-304000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-305000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-305000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-305000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-305000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-305000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-304500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-305500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-305500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-305500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-305500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-305500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-305000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-306000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-306000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-306000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-306000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-306000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-305500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-306500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-306500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-306500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-306500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-306500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-306000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-307000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-307000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-307000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-307000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-307000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-306500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-307500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-307500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-307500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-307500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-307500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-307000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-308000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-308000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-308000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-308000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-308000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-307500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-308500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-308500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-308500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-308500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-308500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-308000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-309000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-309000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-309000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-309000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-309000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-308500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-309500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-309500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-309500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-309500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-309500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-309000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-310000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-310000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-310000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-310000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-310000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-309500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-310500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-310500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-310500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-310500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-310500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-310000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-311000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-311000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-311000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-311000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-311000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-310500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-311500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-311500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-311500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-311500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-311500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-311000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-312000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-312000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-312000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-312000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-312000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-311500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-312500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-312500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-312500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-312500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-312500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-312000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-313000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-313000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-313000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-313000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-313000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-312500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-313500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-313500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-313500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-313500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-313500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-313000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-314000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-314000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-314000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-314000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-314000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-313500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-314500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-314500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-314500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-314500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-314500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-314000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-315000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-315000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-315000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-315000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-315000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-314500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-315500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-315500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-315500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-315500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-315500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-315000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-316000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-316000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-316000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-316000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-316000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-315500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-316500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-316500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-316500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-316500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-316500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-316000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-317000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-317000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-317000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-317000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-317000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-316500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-317500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-317500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-317500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-317500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-317500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-317000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-318000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-318000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-318000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-318000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-318000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-317500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-318500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-318500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-318500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-318500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-318500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-318000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-319000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-319000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-319000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-319000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-319000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-318500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-319500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-319500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-319500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-319500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-319500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-319000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-320000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-320000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-320000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-320000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-320000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-319500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-320500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-320500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-320500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-320500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-320500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-320000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-321000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-321000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-321000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-321000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-321000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-320500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-321500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-321500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-321500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-321500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-321500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-321000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-322000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-322000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-322000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-322000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-322000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-321500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-322500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-322500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-322500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-322500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-322500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-322000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-323000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-323000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-323000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-323000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-323000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-322500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-323500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-323500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-323500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-323500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-323500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-323000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-324000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-324000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-324000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-324000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-324000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-323500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-324500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-324500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-324500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-324500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-324500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-324000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-325000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-325000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-325000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-325000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-325000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-324500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-325500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-325500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-325500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-325500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-325500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-325000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-326000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-326000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-326000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-326000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-326000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-325500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-326500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-326500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-326500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-326500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-326500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-326000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-327000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-327000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-327000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-327000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-327000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-326500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-327500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-327500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-327500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-327500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-327500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-327000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-328000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-328000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-328000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-328000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-328000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-327500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-328500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-328500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-328500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-328500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-328500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-328000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-329000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-329000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-329000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-329000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-329000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-328500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-329500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-329500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-329500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-329500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-329500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-329000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-330000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-330000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-330000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-330000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-330000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-329500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-330500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-330500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-330500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-330500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-330500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-330000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-331000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-331000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-331000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-331000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-331000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-330500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-331500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-331500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-331500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-331500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-331500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-331000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-332000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-332000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-332000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-332000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-332000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-331500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-332500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-332500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-332500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-332500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-332500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-332000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-333000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-333000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-333000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-333000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-333000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-332500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-333500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-333500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-333500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-333500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-333500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-333000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-334000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-334000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-334000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-334000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-334000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-333500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-334500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-334500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-334500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-334500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-334500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-334000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-335000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-335000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-335000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-335000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-335000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-334500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-335500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-335500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-335500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-335500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-335500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-335000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-336000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-336000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-336000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-336000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-336000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-335500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-336500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-336500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-336500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-336500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-336500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-336000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-337000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-337000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-337000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-337000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-337000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-336500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-337500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-337500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-337500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-337500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-337500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-337000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-338000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-338000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-338000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-338000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-338000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-337500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-338500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-338500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-338500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-338500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-338500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-338000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-339000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-339000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-339000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-339000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-339000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-338500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-339500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-339500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-339500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-339500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-339500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-339000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-340000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-340000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-340000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-340000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-340000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-339500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-340500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-340500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-340500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-340500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-340500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-340000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-341000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-341000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-341000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-341000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-341000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-340500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-341500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-341500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-341500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-341500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-341500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-341000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-342000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-342000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-342000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-342000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-342000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-341500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-342500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-342500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-342500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-342500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-342500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-342000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-343000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-343000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-343000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-343000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-343000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-342500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-343500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-343500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-343500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-343500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-343500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-343000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-344000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-344000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-344000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-344000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-344000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-343500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-344500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-344500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-344500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-344500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-344500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-344000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-345000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-345000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-345000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-345000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-345000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-344500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-345500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-345500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-345500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-345500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-345500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-345000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-346000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-346000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-346000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-346000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-346000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-345500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-346500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-346500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-346500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-346500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-346500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-346000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-347000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-347000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-347000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-347000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-347000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-346500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-347500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-347500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-347500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-347500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-347500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-347000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-348000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-348000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-348000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-348000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-348000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-347500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-348500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-348500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-348500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-348500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-348500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-348000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-349000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-349000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-349000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-349000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-349000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-348500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-349500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-349500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-349500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-349500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-349500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-349000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-350000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-350000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-350000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-350000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-350000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-349500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-350500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-350500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-350500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-350500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-350500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-350000] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-351000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-351000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-351000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-351000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-351000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-350500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-351500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-351500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-351500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-351500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-351500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-351000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-352000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-352000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-352000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-352000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-352000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-351500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-352500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-352500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-352500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-352500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-352500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-352000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-353000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-353000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-353000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-353000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-353000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-352500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-353500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-353500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-353500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-353500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-353500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-353000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-354000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-354000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-354000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-354000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-354000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-353500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-354500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-354500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-354500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-354500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-354500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-354000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-355000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-355000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-355000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-355000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-355000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-354500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-355500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-355500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-355500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-355500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-355500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-355000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-356000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-356000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-356000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-356000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-356000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-355500] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-356500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-356500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-356500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-356500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-356500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-356000] due to args.save_total_limit\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-357000\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-357000/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-357000/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-357000/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-357000/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-356500] due to args.save_total_limit\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1357: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  args.max_grad_norm,\n",
            "Saving model checkpoint to t5-small-finetuned-xsum-proplus/checkpoint-357500\n",
            "Configuration saved in t5-small-finetuned-xsum-proplus/checkpoint-357500/config.json\n",
            "Model weights saved in t5-small-finetuned-xsum-proplus/checkpoint-357500/pytorch_model.bin\n",
            "tokenizer config file saved in t5-small-finetuned-xsum-proplus/checkpoint-357500/tokenizer_config.json\n",
            "Special tokens file saved in t5-small-finetuned-xsum-proplus/checkpoint-357500/special_tokens_map.json\n",
            "Deleting older checkpoint [t5-small-finetuned-xsum-proplus/checkpoint-357000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 44687\n",
            "  Batch size = 10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='357500' max='357500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [357500/357500 17:01:02, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.112600</td>\n",
              "      <td>0.954671</td>\n",
              "      <td>38.804800</td>\n",
              "      <td>22.345000</td>\n",
              "      <td>33.817800</td>\n",
              "      <td>33.812000</td>\n",
              "      <td>18.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.945700</td>\n",
              "      <td>0.818165</td>\n",
              "      <td>40.455600</td>\n",
              "      <td>23.730400</td>\n",
              "      <td>35.520400</td>\n",
              "      <td>35.516400</td>\n",
              "      <td>18.273100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.864100</td>\n",
              "      <td>0.754082</td>\n",
              "      <td>41.806300</td>\n",
              "      <td>24.592600</td>\n",
              "      <td>36.460600</td>\n",
              "      <td>36.451800</td>\n",
              "      <td>18.476600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.813200</td>\n",
              "      <td>0.716817</td>\n",
              "      <td>41.965900</td>\n",
              "      <td>24.741900</td>\n",
              "      <td>36.829600</td>\n",
              "      <td>36.827400</td>\n",
              "      <td>18.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.785400</td>\n",
              "      <td>0.691219</td>\n",
              "      <td>42.227300</td>\n",
              "      <td>24.909200</td>\n",
              "      <td>37.112000</td>\n",
              "      <td>37.107300</td>\n",
              "      <td>18.461100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.766100</td>\n",
              "      <td>0.672171</td>\n",
              "      <td>42.813300</td>\n",
              "      <td>25.417300</td>\n",
              "      <td>37.697900</td>\n",
              "      <td>37.691700</td>\n",
              "      <td>18.380600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.739700</td>\n",
              "      <td>0.659200</td>\n",
              "      <td>44.070700</td>\n",
              "      <td>26.657300</td>\n",
              "      <td>38.743000</td>\n",
              "      <td>38.739000</td>\n",
              "      <td>18.235500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.723000</td>\n",
              "      <td>0.647439</td>\n",
              "      <td>43.527400</td>\n",
              "      <td>26.367700</td>\n",
              "      <td>38.294800</td>\n",
              "      <td>38.289300</td>\n",
              "      <td>18.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.714100</td>\n",
              "      <td>0.638523</td>\n",
              "      <td>43.870000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>38.746600</td>\n",
              "      <td>38.743000</td>\n",
              "      <td>18.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.704800</td>\n",
              "      <td>0.631282</td>\n",
              "      <td>44.403200</td>\n",
              "      <td>26.967700</td>\n",
              "      <td>39.172800</td>\n",
              "      <td>39.170800</td>\n",
              "      <td>18.261500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.695600</td>\n",
              "      <td>0.624862</td>\n",
              "      <td>44.217700</td>\n",
              "      <td>26.759700</td>\n",
              "      <td>39.039400</td>\n",
              "      <td>39.038200</td>\n",
              "      <td>18.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.680100</td>\n",
              "      <td>0.620151</td>\n",
              "      <td>44.199000</td>\n",
              "      <td>26.814600</td>\n",
              "      <td>39.030800</td>\n",
              "      <td>39.031200</td>\n",
              "      <td>18.374900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.684900</td>\n",
              "      <td>0.615456</td>\n",
              "      <td>44.413700</td>\n",
              "      <td>26.863100</td>\n",
              "      <td>39.170900</td>\n",
              "      <td>39.168600</td>\n",
              "      <td>18.334300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.675800</td>\n",
              "      <td>0.612161</td>\n",
              "      <td>44.401100</td>\n",
              "      <td>26.978200</td>\n",
              "      <td>39.220200</td>\n",
              "      <td>39.216300</td>\n",
              "      <td>18.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.672000</td>\n",
              "      <td>0.609685</td>\n",
              "      <td>44.473200</td>\n",
              "      <td>27.065900</td>\n",
              "      <td>39.301600</td>\n",
              "      <td>39.298900</td>\n",
              "      <td>18.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.672400</td>\n",
              "      <td>0.607404</td>\n",
              "      <td>44.823800</td>\n",
              "      <td>27.255900</td>\n",
              "      <td>39.582000</td>\n",
              "      <td>39.576400</td>\n",
              "      <td>18.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.672700</td>\n",
              "      <td>0.605206</td>\n",
              "      <td>44.698700</td>\n",
              "      <td>27.227000</td>\n",
              "      <td>39.507500</td>\n",
              "      <td>39.502000</td>\n",
              "      <td>18.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.670300</td>\n",
              "      <td>0.604301</td>\n",
              "      <td>45.039000</td>\n",
              "      <td>27.494100</td>\n",
              "      <td>39.765900</td>\n",
              "      <td>39.763700</td>\n",
              "      <td>18.284000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.667100</td>\n",
              "      <td>0.603407</td>\n",
              "      <td>44.720900</td>\n",
              "      <td>27.231100</td>\n",
              "      <td>39.488600</td>\n",
              "      <td>39.485000</td>\n",
              "      <td>18.334300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.664200</td>\n",
              "      <td>0.603232</td>\n",
              "      <td>44.755300</td>\n",
              "      <td>27.255800</td>\n",
              "      <td>39.505700</td>\n",
              "      <td>39.503500</td>\n",
              "      <td>18.329200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=357500, training_loss=0.7705388072620739, metrics={'train_runtime': 61262.292, 'train_samples_per_second': 58.355, 'train_steps_per_second': 5.836, 'total_flos': 3.664206576601006e+17, 'train_loss': 0.7705388072620739, 'epoch': 20.0})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Jf63OLqPUAMx",
        "outputId": "18fbd72d-a5ad-4bfa-a686-55b36ad8f7ae"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ce2Bff5HDkmP",
        "outputId": "2ddc3853-4890-4fa7-c588-8646488d65ab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKpK6V4UZUN"
      },
      "source": [
        "! cp -R /content/t5-small-finetuned-xsum-proplus /content/drive/MyDrive/t5-small-finetuned-xsum-proplus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvk0aDBSUqLJ",
        "outputId": "93d52472-8ec6-411d-ca43-1d6bab000387"
      },
      "source": [
        "!ls -ltr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 489580\n",
            "drwxr-xr-x 1 root root      4096 Oct 26 13:34 sample_data\n",
            "drwxr-xr-x 4 root root      4096 Nov  2 00:50 t5-small-finetuned-xsum-gcloud1\n",
            "-rw-r--r-- 1 root root 501306129 Nov  2 02:00 augmented_recipes.csv\n",
            "drwxr-xr-x 5 root root      4096 Nov  2 18:42 t5-small-finetuned-xsum-proplus\n",
            "drwx------ 6 root root      4096 Nov  2 20:22 drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCI3nvZ7UrxX",
        "outputId": "22365467-bd39-4094-d39b-424a87ef7ab6"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlx1ZXp8UtU7"
      },
      "source": [
        "!cd /content/t5-small-finetuned-xsum-proplus/checkpoint-357500/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0FTwyiDYGRN"
      },
      "source": [
        "!cd /content/t5-small-finetuned-xsum-proplus/checkpoint-357500/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieE86l2_YOlk",
        "outputId": "e755b11a-2a49-4ef9-d4be-33531327b33d"
      },
      "source": [
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNwpn327YwXe"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpTM6uP7YfZK"
      },
      "source": [
        "DIRECTORY = '/content/t5-small-finetuned-xsum-proplus/' \n",
        "\n",
        "if os.getcwd() != DIRECTORY:\n",
        "  os.chdir(DIRECTORY)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQyhiWKaY2Qb",
        "outputId": "9a2fc68d-cf31-45bc-ca40-10ffd1f76ad8"
      },
      "source": [
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 3 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   ../runs/Nov02_02-01-23_881d6d578c6e/events.out.tfevents.1635818494.881d6d578c6e.1767.0\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f-d0knnY4Ni",
        "outputId": "f5bc6bf1-aeaf-47a6-e2f8-aa141fd29898"
      },
      "source": [
        "!git log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mcommit eda00e7ee51613089f743a179580fc26bb127ace\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m\n",
            "Author: R-M <teepika.ramasamymarimuthu@sjsu.edu>\n",
            "Date:   Tue Nov 2 02:02:31 2021 +0000\n",
            "\n",
            "    Training in progress, step 500\n",
            "\n",
            "\u001b[33mcommit 4d6dccc4053bca2e405030f228fcc61e96f786e3\u001b[m\n",
            "Author: R-M <teepika.ramasamymarimuthu@sjsu.edu>\n",
            "Date:   Tue Nov 2 01:02:26 2021 +0000\n",
            "\n",
            "    Training in progress, step 500\n",
            "\n",
            "\u001b[33mcommit e7d059cbc704a7b7d6ed18e1365f2a9bc1bdc274\u001b[m\n",
            "Author: R-M <teepika.ramasamymarimuthu@sjsu.edu>\n",
            "Date:   Tue Nov 2 00:54:00 2021 +0000\n",
            "\n",
            "    Training in progress, step 500\n",
            "\n",
            "\u001b[33mcommit 698dca2d9f5fd77a474bde944a12da68e3a391fc\u001b[m\u001b[33m (\u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m\n",
            "Author: system <system@huggingface.co>\n",
            "Date:   Tue Nov 2 00:52:07 2021 +0000\n",
            "\n",
            "    initial commit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0H986zgZLLb",
        "outputId": "3dbb53ef-9284-4cfd-e505-ab13b4bf64e2"
      },
      "source": [
        "!git push https://user:$(cat /root/.huggingface/token)@huggingface.co/Teepika/t5-small-finetuned-xsum-proplus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rGit LFS: (0 of 0 files, 18 skipped) 0 B / 0 B, 692.67 MB skipped\rGit LFS: (0 of 0 files, 18 skipped) 0 B / 0 B, 692.67 MB skippedCounting objects: 44, done.\n",
            "Delta compression using up to 8 threads.\n",
            "Compressing objects:   2% (1/43)   \rCompressing objects:   4% (2/43)   \rCompressing objects:   6% (3/43)   \rCompressing objects:   9% (4/43)   \rCompressing objects:  11% (5/43)   \rCompressing objects:  13% (6/43)   \rCompressing objects:  16% (7/43)   \rCompressing objects:  18% (8/43)   \rCompressing objects:  20% (9/43)   \rCompressing objects:  23% (10/43)   \rCompressing objects:  25% (11/43)   \rCompressing objects:  27% (12/43)   \rCompressing objects:  30% (13/43)   \rCompressing objects:  32% (14/43)   \rCompressing objects:  34% (15/43)   \rCompressing objects:  37% (16/43)   \rCompressing objects:  39% (17/43)   \rCompressing objects:  41% (18/43)   \rCompressing objects:  44% (19/43)   \rCompressing objects:  46% (20/43)   \rCompressing objects:  48% (21/43)   \rCompressing objects:  51% (22/43)   \rCompressing objects:  53% (23/43)   \rCompressing objects:  55% (24/43)   \rCompressing objects:  58% (25/43)   \rCompressing objects:  60% (26/43)   \rCompressing objects:  62% (27/43)   \rCompressing objects:  65% (28/43)   \rCompressing objects:  67% (29/43)   \rCompressing objects:  69% (30/43)   \rCompressing objects:  72% (31/43)   \rCompressing objects:  74% (32/43)   \rCompressing objects:  76% (33/43)   \rCompressing objects:  79% (34/43)   \rCompressing objects:  81% (35/43)   \rCompressing objects:  83% (36/43)   \rCompressing objects:  86% (37/43)   \rCompressing objects:  88% (38/43)   \rCompressing objects:  90% (39/43)   \rCompressing objects:  93% (40/43)   \rCompressing objects:  95% (41/43)   \rCompressing objects:  97% (42/43)   \rCompressing objects: 100% (43/43)   \rCompressing objects: 100% (43/43), done.\n",
            "Writing objects:   2% (1/44)   \rWriting objects:   4% (2/44)   \rWriting objects:   6% (3/44)   \rWriting objects:   9% (4/44)   \rWriting objects:  11% (5/44)   \rWriting objects:  13% (6/44)   \rWriting objects:  15% (7/44)   \rWriting objects:  18% (8/44)   \rWriting objects:  20% (9/44)   \rWriting objects:  22% (10/44)   \rWriting objects:  25% (11/44)   \rWriting objects:  27% (12/44)   \rWriting objects:  29% (13/44)   \rWriting objects:  31% (14/44)   \rWriting objects:  34% (15/44)   \rWriting objects:  36% (16/44)   \rWriting objects:  38% (17/44)   \rWriting objects:  40% (18/44)   \rWriting objects:  43% (19/44)   \rWriting objects:  45% (20/44)   \rWriting objects:  47% (21/44)   \rWriting objects:  50% (22/44)   \rWriting objects:  52% (23/44)   \rWriting objects:  54% (24/44)   \rWriting objects:  56% (25/44)   \rWriting objects:  59% (26/44)   \rWriting objects:  61% (27/44)   \rWriting objects:  63% (28/44)   \rWriting objects:  65% (29/44)   \rWriting objects:  68% (30/44)   \rWriting objects:  70% (31/44)   \rWriting objects:  72% (32/44)   \rWriting objects:  75% (33/44)   \rWriting objects:  77% (34/44)   \rWriting objects:  79% (35/44)   \rWriting objects:  81% (36/44)   \rWriting objects:  84% (37/44)   \rWriting objects:  86% (38/44)   \rWriting objects:  88% (39/44)   \rWriting objects:  90% (40/44)   \rWriting objects:  93% (41/44)   \rWriting objects:  95% (42/44)   \rWriting objects:  97% (43/44)   \rWriting objects: 100% (44/44)   \rWriting objects: 100% (44/44), 558.49 KiB | 5.76 MiB/s, done.\n",
            "Total 44 (delta 6), reused 0 (delta 0)\n",
            "To https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus\n",
            "   698dca2..eda00e7  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28umb9WuZj7I",
        "outputId": "4ade80f6-6bab-4c67-de84-5045b5235a3b"
      },
      "source": [
        "!shasum -a 512 config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a6566bb93b8983776be0e960552aeaaf2d13a482a7bf1f8ad7d7b257b0ec82f469d9bc732ee91ad33fe4fe0ab8e053c85b5dbece9e1b04f9c741b072fac83945  config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfbcMG4BaE5U",
        "outputId": "d0dbdabe-c83f-4367-918d-30906e7ec718"
      },
      "source": [
        "!shasum -a 512 ../config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a6566bb93b8983776be0e960552aeaaf2d13a482a7bf1f8ad7d7b257b0ec82f469d9bc732ee91ad33fe4fe0ab8e053c85b5dbece9e1b04f9c741b072fac83945  ../config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xblVjU-al65",
        "outputId": "3696ccdf-af80-41b8-be8d-7954f7fbc43c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/t5-small-finetuned-xsum-proplus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25JzOzjvavtL",
        "outputId": "6c70eb9b-73d0-4475-db40-5588a175beaa"
      },
      "source": [
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 3 commits.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   runs/Nov02_02-01-23_881d6d578c6e/events.out.tfevents.1635818494.881d6d578c6e.1767.0\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "limQbiRQa2yC",
        "outputId": "f86aa3c1-4c7a-42d1-b7ab-519200c394ba"
      },
      "source": [
        "!ls -ltr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 237792\n",
            "drwxr-xr-x 7 root root      4096 Nov  2 02:01 runs\n",
            "-rw-r--r-- 1 root root      1385 Nov  2 02:02 config.json\n",
            "-rw-r--r-- 1 root root      1924 Nov  2 02:02 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root 242085627 Nov  2 02:02 pytorch_model.bin\n",
            "-rw-r--r-- 1 root root      1786 Nov  2 02:02 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root   1387489 Nov  2 02:02 tokenizer.json\n",
            "-rw-r--r-- 1 root root      2991 Nov  2 02:02 training_args.bin\n",
            "drwxr-xr-x 2 root root      4096 Nov  2 18:42 checkpoint-357500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uA45ydfJa5j_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e6abc45370b54ae592ca96d289ec3550",
            "bf56ec1da36f4f4ba9bc091e67e116a6",
            "f1a726683152444394aba081b8421937",
            "ddb295ab10cc4dc8b43a8ecdf87f6ba4",
            "d36973d88bdd4097847b800963617a41",
            "e0b0d0f9046e41a2ba3fc840bef60e3e",
            "55c6a49cdfc44a73a478c912960610e6",
            "ab8211615c1747b1b7350ed5161c512d",
            "8bf6932f2fc44d22827a73ba9bd1adf5",
            "610c561f4b9f481eaf10a6bc740e3916",
            "3c1c9f7741ce4d18b26a7536628e1d7f",
            "532755844e8c4175b7d062f0924cfd56",
            "c073038cbff845c5ab26b36600a655ac",
            "cba78855e6de433b8392fc1cf4ee6327",
            "9866f66d9f494b0698a5dd1e68046efe",
            "54c9e54b88134f1db61767506cc81850",
            "83aae670e561443e88495a851c2c12a6",
            "1820eacf17814f78bc6191ad7955ae29",
            "703a8fcb675d48e8899d3ff0a443c0e8",
            "6a7460a06987451da49ed52ff0a6522b",
            "a94a7f20e50e4e42afbb0ffaa750c8b7",
            "339a10d3f5d44544823177de6754316f",
            "317226337f104f85838f149f5a3120cc",
            "61ae2a271dac40b5a5bbb1c90699e041",
            "bbc6a69d5e4b419d8e43ed4d9698f936",
            "a03e9de71e91458d8339fde7d79733fc",
            "3cb35c3fa66c49eca3f7f1676280954f",
            "f125ef60ec9e4aacb97085133454e528",
            "9f5c4a23ef744790af7e3b15a56a909d",
            "6d884104cd584ab5b7d567c5ce25c2ba",
            "2a057ea455c4452fa944b36beaa3edd8",
            "b18cc73838ce455c9f3fbf4537b30ed9",
            "ee4be3f989c049b5a1903a4c47c6b1f9",
            "cb3e420ed9564066b5cd48528a5d2c1c",
            "7983487b932b48f5b4f538142c76cfe5",
            "fd25fb426f214fd293d7230185bb4042",
            "bedf5a64f9fe4daf90132bc9557317c0",
            "7c86ec259e64456e8a3405cfd8906de6",
            "95e922a2d4214db380adfffea87f336b",
            "b20686d333c84c80a3c33b43f70fb8a6",
            "e5e94c5644cf4afeb24e67301040fcae",
            "0c373476e3214d9eac1989ab205e0915",
            "7d7db422f91f4a399cbfcd9449da26df",
            "875f1fbfe90b4d9ca4bcb222ecf951d0",
            "b9d93d9b665c4664b467d297200e2b6a",
            "d6af24d24662465e9ef20822201f0aa2",
            "528a460d0eaa411e811f9778a953ab0b",
            "9ec61a911fb940fa8532df6c13de9b79",
            "117f45d6762549728a3ae825aef53e4d",
            "5a19411f6bb9463aa7fa084b78a58bd0",
            "9966b85fda9345558e6df2106ae88ab7",
            "44fcbec5422145dca4952e3dc800c15b",
            "ca7ab4997ce74815b868c1234eefd771",
            "68b85ca392d84fd2b4bb0c64578371e1",
            "8234684421db4022a20b2140384c3bae"
          ]
        },
        "outputId": "7625ea5e-df4a-4b27-92dc-6c3e4dc17160"
      },
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\", model=\"Teepika/t5-small-finetuned-xsum-proplus\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1oee4k4y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6abc45370b54ae592ca96d289ec3550",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e2c4ca5f8cb2a7952d22542861920179cddfc82df7ff4504cdc74de183a16a34.6d4448604da4186d665c8b8853330ba7e01cdde8f8cfa1b9d269e2380b0d7694\n",
            "creating metadata file for /root/.cache/huggingface/transformers/e2c4ca5f8cb2a7952d22542861920179cddfc82df7ff4504cdc74de183a16a34.6d4448604da4186d665c8b8853330ba7e01cdde8f8cfa1b9d269e2380b0d7694\n",
            "loading configuration file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e2c4ca5f8cb2a7952d22542861920179cddfc82df7ff4504cdc74de183a16a34.6d4448604da4186d665c8b8853330ba7e01cdde8f8cfa1b9d269e2380b0d7694\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e2c4ca5f8cb2a7952d22542861920179cddfc82df7ff4504cdc74de183a16a34.6d4448604da4186d665c8b8853330ba7e01cdde8f8cfa1b9d269e2380b0d7694\n",
            "Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5rl34zlt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "532755844e8c4175b7d062f0924cfd56",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/612fcdcb7ac8385ce4930c895bfee58ca2aad51273634474240d79968a7c34e3.07ca42e2f4857faf7a39e7e4c451be127ba15e9242030381c5ae1602a8736af4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/612fcdcb7ac8385ce4930c895bfee58ca2aad51273634474240d79968a7c34e3.07ca42e2f4857faf7a39e7e4c451be127ba15e9242030381c5ae1602a8736af4\n",
            "loading weights file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/612fcdcb7ac8385ce4930c895bfee58ca2aad51273634474240d79968a7c34e3.07ca42e2f4857faf7a39e7e4c451be127ba15e9242030381c5ae1602a8736af4\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Teepika/t5-small-finetuned-xsum-proplus.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpe5422qwn\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "317226337f104f85838f149f5a3120cc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.88k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/b5631308a32db6b3c8d4a468aecb97521a2c297da7f63b379a25db5ae19ffa61.b36b700ce91a4b99604e51a8f1694160d98bbbe9d04aee9a964bd9c86d8e4c2e\n",
            "creating metadata file for /root/.cache/huggingface/transformers/b5631308a32db6b3c8d4a468aecb97521a2c297da7f63b379a25db5ae19ffa61.b36b700ce91a4b99604e51a8f1694160d98bbbe9d04aee9a964bd9c86d8e4c2e\n",
            "https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2zsa8jqh\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb3e420ed9564066b5cd48528a5d2c1c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/aaaefe5363f4a8e16403ef0c36152929b424979c72f0cd994b004596d4e8f21b.d5d9962ef900835aadd4da96cb73224a1935218df57fc612b0585098280be3b4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/aaaefe5363f4a8e16403ef0c36152929b424979c72f0cd994b004596d4e8f21b.d5d9962ef900835aadd4da96cb73224a1935218df57fc612b0585098280be3b4\n",
            "https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5j1vyf1c\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9d93d9b665c4664b467d297200e2b6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "storing https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/6c453b3069e61dc8500711688273f7f6510be92a0e251fabfe52b37bb1890612.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "creating metadata file for /root/.cache/huggingface/transformers/6c453b3069e61dc8500711688273f7f6510be92a0e251fabfe52b37bb1890612.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "loading file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/spiece.model from cache at None\n",
            "loading file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/aaaefe5363f4a8e16403ef0c36152929b424979c72f0cd994b004596d4e8f21b.d5d9962ef900835aadd4da96cb73224a1935218df57fc612b0585098280be3b4\n",
            "loading file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6c453b3069e61dc8500711688273f7f6510be92a0e251fabfe52b37bb1890612.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
            "loading file https://huggingface.co/Teepika/t5-small-finetuned-xsum-proplus/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/b5631308a32db6b3c8d4a468aecb97521a2c297da7f63b379a25db5ae19ffa61.b36b700ce91a4b99604e51a8f1694160d98bbbe9d04aee9a964bd9c86d8e4c2e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37Tje8MnuNPb",
        "outputId": "3bf1956f-d490-4407-bb98-f07e9fc258fd"
      },
      "source": [
        "summarizer([\"*NOTE: Two separate seasoning suggestions are given. Make a choice and proceed with recipe.Depending on size of squash, cut into half or fourths. Remove seeds.For spicy squash, drizzle (roughly 1/2 teaspoon) olive oil or melted butter over each cut squash piece. Season with Mexican Seasoning Mix II. For sweet squash, drizzle (roughly 1/2 teaspoon) melted honey, butter, grated piloncillo (or any combination of) over each cut squash piece. Season with Sweet Mexican Spice Mix. Bake at 350 degrees, again depending on size, for 40 minutes up to an hour, until a fork can easily pierce the skin. Be careful not to burn the squash especially if you opt to use sugar or butter. If you feel more comfortable, cover the squash with aluminum foil the first half hour, give or take, of baking.If desired, season with salt. Note for Vegan use piloncillo.\",\"Preheat oven to 425 degrees F.Press dough into the bottom and sides of a 12 inch pizza pan. Bake for 5 minutes until set but not browned. Cut sausage into small pieces. Whisk eggs and milk in a bowl until frothy. Spoon sausage over baked crust and sprinkle with cheese. Pour egg mixture slowly over sausage and cheese. S& P to taste. Bake 15-20 minutes or until eggs are set and crust is brown.\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n",
            "Your max_length is set to 200, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'appetizers squash low-cholesterol low-saturated-fat low-protein low-carb low-calorie low-fat high-protein high-fat easy'},\n",
              " {'summary_text': 'main-dish 30-minutes-or-less low-cholesterol low-saturated-fat low-protein low-carb low-fat easy'}]"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    }
  ]
}